{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 RNN 구현            \n",
    "\n",
    "### 1. 목표\n",
    "\n",
    "- 우리가 지금부터 구현할 것은 결국 '가로 방향으로 성장한 신경망'!       \n",
    "\n",
    "![rnn1](./PostingPic/rnn_1.png)       \n",
    "\n",
    "\n",
    "- 길이가 T인 시계열 데이터를 받아서, 각 시각의 은닉 상태를 T개 출력한다.\n",
    "- 상하 방향의 입력과, 출력을 하나로 묶으면 옆으로 늘어선 일련의 계층을 하나의 계층으로 간주할 수 있다.  \n",
    "- 즉, 묶어주는 $Xs$ 로 묶여진 $Hs$를 출력하는 하나의 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 구현           \n",
    "\n",
    "- 한 단위의 RNN 클래스를 구현해보자.    \n",
    "- 여기서, 우리는 데이터를 미니배치로 처리한다.      \n",
    "\n",
    "![rnn2](./PostingPic/rnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ht(한 계층의 출력) = tahn(ht-1 * Wh + Xt * Wx + b)      \n",
    "\n",
    "> - h(t-1) * Wh : 이전 단계의 h값과, Wh(h 출력을 내기 위한 가중치 세트)      \n",
    "> - Xt * Wx : 현재 단계의 인풋 Xt와 Wx(h값을 내기 위한 가중치 세트)        \n",
    "> - b : 편향    \n",
    "\n",
    "##### 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    #파라미터로 Wx, Wh의 두 값이 필요\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params[Wx, Wh, b]\n",
    "        \n",
    "        #가중치는 요렇게\n",
    "        #동일 크기의 제로 행렬을 만들어준다.\n",
    "        self.grads[np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        \n",
    "        #다음 계층으로 넘어갈 때 기억해야 할 값인데\n",
    "        #현재는 아무것도 알 수 없으므로 None\n",
    "        self.cache = None\n",
    "        \n",
    "    #순방향 전파\n",
    "    #x(각 시각별 인풋)\n",
    "    #h_prev(이전 계층의 출력)\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        \n",
    "        # 이 부분이 바로 그 그림입니다\n",
    "        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        \n",
    "        #이번 층에서 나온 결과값인 h_next를 리턴\n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![순전파그래프](./PostingPic/순전파그래프.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 역전파   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파일때는 앞서있었던 그 값\n",
    "# 역전파일때는 거꾸로 온겁니다!\n",
    "def backward(self, dh_next):\n",
    "    Wx, Wh, b = self.params\n",
    "    x, h_prev, h_next = self.cache\n",
    "    \n",
    "    dt = dh_next * (1-h_next ** 2)\n",
    "    \n",
    "    #편향의 d\n",
    "    db = np.sum(dt, axis=0)\n",
    "    \n",
    "    #Wh의 d\n",
    "    dWh = np.matmul(h_prev.T, dt)\n",
    "    \n",
    "    #h_prev의 d\n",
    "    dh_prev = np.matmul(dt, Wh.T)\n",
    "    \n",
    "    #Wx의 d\n",
    "    dWx = np.matmul(x.T, dt)\n",
    "    \n",
    "    #x의 d\n",
    "    dx = np.matmul(dt, Wx.T)\n",
    "    \n",
    "    #전달해야 할 기울기 Wx, Wh, b\n",
    "    self.grad[0][] = dWx\n",
    "    self.grad[1][] = dWh\n",
    "    self.grad[2][] = db\n",
    "    \n",
    "    return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 TimeRnn계층의 구현                 \n",
    "\n",
    "- Time Rnn계층 == Rnn * T개         \n",
    "- Rnn 계층의 은닉 상태인 h를 인스턴스 변수로 사용    \n",
    "\n",
    "![TimeRnn](./PostingPic/TimeRnn.png)            \n",
    "\n",
    "\n",
    "- 이 책에서는 '은닉 상태를 인계받을지'를 stateful 이라는 인수로 조정한다.      \n",
    "- Stateful Stateless\n",
    "\n",
    "![예제](./PostingPic/State.png)             \n",
    "\n",
    "\n",
    "##### 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        #이번 계층의 결과물, h의 기울기\n",
    "        self.h, self.dh = None, None\n",
    "        \n",
    "        #연결 상태는 어떻게 할거니?\n",
    "        self.stateful = stateful\n",
    "        \n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 계층은 여러 개의 x, h를 묶은 것이라는 것을 기억하자!\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        #N = 미니배치, T=rnn개수, D=입력 벡터의 차원 수 \n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        #레이어를 리스트로 담아둘거야.\n",
    "        self.layers = []\n",
    "        \n",
    "        #결과물을 담아둘 수 있는 행렬을 만듦\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        #스테이트가 true이고 h가 설정되어 있지 않으면\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            # *는 리스트의 원소를 추출하여 메서드의 인수로 전달한다.\n",
    "            # self.params = [Wx, Wh, b]\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:,t,:], self.h)\n",
    "            \n",
    "            #  N, T, H\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이렇게](./PostingPic/rnn_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 역전파           \n",
    "\n",
    "- 우리는 Truncated BPTT(잘린 BPTT)를 수행하므로, 이전 시각 역전파는 기억하지 않는다. (한 단위에서 끝)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이렇게](./PostingPic/TimeRnn_bptt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self, dhs):\n",
    "    Wx, Wh, b = self.params\n",
    "    \n",
    "    # 기울기 셰입\n",
    "    N, T, H = dhs.shpae\n",
    "    D, H = Wx.shape\n",
    "    \n",
    "    #모든 기울기는 0으로 초기화 세팅\n",
    "    dxs = np.empty((N,T,D), dtype='f')\n",
    "    dh = 0\n",
    "    grads =[0,0,0]\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        layer = self.layers[t]\n",
    "        \n",
    "        dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "        \n",
    "        #전체 인풋 xs에 대한 기울기\n",
    "        dxs = dx\n",
    "        \n",
    "        for i, grad in enumerate(layer.grads):\n",
    "            grads[i] +=  grad\n",
    "            \n",
    "    for i, grad in enumerate(layer.grads):\n",
    "        self.grads[i][] = grad\n",
    "    \n",
    "    self.dh = dh\n",
    "    \n",
    "    return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단, 순전파 시에 갈라졌던 부분들을 합산해서 적용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 시계열 데이터 처리 계층 구현          \n",
    "\n",
    "- RNNLM (RNN + LM)       \n",
    "\n",
    "![모델](./PostingPic/언어모델.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 층 : 단어 ID를 단어 벡터(분산표현)으로 변환         \n",
    "- RNN : 우리가 지금까지 한 것      \n",
    "- Affine, Softmax : 마지막 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![여기](./PostingPic/5-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Say를 보자구   \n",
    "- hello, goodbye 둘 다 확률이 높게 나옴(둘 다 정답이 될 만한 단어들)        \n",
    "- You say goodbye and I say hello  의 순서를 __기억하고__ 있다는 것         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Time embedding 구현, Time Affine 구현              \n",
    "\n",
    "[전체 코드는 여기에](https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/common/time_layers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. simple RnnLm                \n",
    "\n",
    "- 4개의 계층을 쌓은 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.time_layers import *\n",
    "\n",
    "#F: 포트란형식 Index 순서, 앞 차원부터 변경하고 뒷 쪽 차원을 변경\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN과 Affine 계층에서 Xavier 초깃값 (6.2 가중치의 초깃값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 언어모델의 평가                 \n",
    "\n",
    "##### 퍼플렉시티(Perplexity, 혼란도)           \n",
    "- 확률의 역수         \n",
    "- 각 언어 벡터에 대한 정답 확률을, 거꾸로 생각해보는 것       \n",
    "\n",
    "![퍼플렉시티](./PostingPic/퍼플렉시티.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 직관적으로 생각할 때, 분기 수로 해석할 수 있다.           \n",
    "- 내가 고를 수 있는 선택지가 몇 개인가?             \n",
    "- 정답 확률이 낮은 경우(0.1퍼일 때) = > 퍼플렉시티는 10. 이는 곧 선택가능한 선택지 10개를 주며 이중에 정답이 있어~ 하는 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
