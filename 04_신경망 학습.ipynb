{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04_신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 데이터에서 학습한다.      \n",
    "   1) 학습이란? : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것.     \n",
    "   2) 손실 함수 : 신경망이 학습할 수 있도록 하는 지표     \n",
    "   3) 학습의 목표 : 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것    \n",
    "      * 그 중의 한 방법으로 '함수의 기울기를 활용하는 경사법'      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1-1. \n",
    "\n",
    "1) 신경망의 특징    \n",
    "  - 데이터를 보고 학습할 수 있다.(매개변수가 자동으로 결정된다.)      \n",
    "  - 실제 신경망에서는 수천~수만개의 매개변수가 있다.     \n",
    "  \n",
    "2) 데이터 주도 학습    \n",
    "  - 기계학습에서는 규칙을 찾기 보다는 '데이터에서 기반하여' 문제를 풀어나간다. \n",
    "  - 이미지의 '특징'을 추출하고, 특징의 패턴을 기계학습 기술로 학습한다.      \n",
    "  - 이미지의 특징은 주로 벡터로 기술하며, SIFT, SURF, HOG등의 특징을 활용한다.    \n",
    "  - 이런 특징을 이용하여 이미지를 벡터로 변환하고, 변환된 벡터를 지도학습 방식의 대표적인 방법인 SVM, KNN 등으로 학습한다.      \n",
    "  ---\n",
    "  - 단, __특징을 찾는 방법은 여전히 사람이 설계하므로__ 적절한 특징을 찾고 적용해야 좋은 결과를 얻는다.    \n",
    "  * 딥러닝은 종단간 기계학습(처음부터 끝까지 사람의 개입 없다)이라고도 한다.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) 훈련 데이터와 시험 데이터\n",
    "\n",
    "- 기계학습 문제는 일반적으로 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행하는 것이 일반적이다.    \n",
    "- 훈련 데이터로 최적의 매개변수를 찾는다.      \n",
    "- 시험 데이터로 훈련한 모델의 실력을 평가한다.   \n",
    "> __훈련과 시험 과정을 거쳐 '범용적으로' 사용할 수 있는 모델__ 을 찾는다.      \n",
    "\n",
    "- 훈련 데이터에 오버피팅(과적합) 되지 않도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) __손실 함수__    \n",
    "\n",
    "- 신경망 학습(최적의 매개변수를 찾는 여정)에서 활용하는 평가 지표.    \n",
    "- 일반적으로 평균 제곱 오차와 교차 엔트로피 교차를 사용한다.  \n",
    "\n",
    "    4-1) 평균 제곱 오차   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true :  0.09750000000000003\n",
      "false :  0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#평균 제곱 오차\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "#정답이 2일 때\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "#(2일 확률이 가장 높다고 예측했을 때)\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "true_value = mean_squared_error(np.array(y), np.array(t))\n",
    "\n",
    "#(7일 확률이 가장 높다고 예측했을 때)\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "false_value = mean_squared_error(np.array(y), np.array(t))\n",
    "\n",
    "print(\"true : \", true_value)\n",
    "print(\"false : \", false_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * 제대로 예측했을 때와 잘못 예측했을 때의 오차 확률이 크게 차이남을 알 수 있다.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4-2) 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 밑이 e인 자연로그    \n",
    "   - 정답일 때의 추정(t가 1일때 y)의 자연로그를 계산하는 식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아주 작은 값인 delta를 구해서 log윗부분 이 0이 되지 않도록\n",
    "\n",
    "def cross_entropy(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true :  0.09750000000000003\n",
      "false :  0.5975\n"
     ]
    }
   ],
   "source": [
    "#정답이 2일 때\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "\n",
    "#(2일 확률이 가장 높다고 예측했을 때)\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "true = cross_entropy(np.array(y), np.array(t))\n",
    "\n",
    "#(7일 확률이 가장 높다고 예측했을 때)\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "false = cross_entropy(np.array(y), np.array(t))\n",
    "\n",
    "print(\"true : \", true_value)\n",
    "print(\"false : \", false_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   4-3) 미니배치 학습     \n",
    "   - 지금까지 본 함수들은 각 데이터 1개에 대한 손실 함수였다.    \n",
    "   - 그럼 이제 '전체 데이터에 대한 손실'을 생각한다면?    \n",
    "   \n",
    "   > (-0.5) * 시그마(0부터 끝까지)(데이터 1개에 대한 교차 엔트로피 함수)    \n",
    "   ---\n",
    "   - 하지만 이렇게 모든 데이터에 대한 손실을 계산하려 한다면, 비용이 많이 든다.   \n",
    "   - 데이터 일부를 추려 전체의 '근사치' 로 이용한다.    \n",
    "   - 이러한 학습 방법을 '미니배치' 라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from datasets.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "#무작위로 10장만 빼내기\n",
    "train_size = x_train.shape[0]\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "#random.choice(총 넘버 수, 뽑을 사이즈)\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 하나당 교차 엔트로피를 구하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#배치용 교차 엔트로피 오차 구하기 함수\n",
    "def cross_entropy_batch_each(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.num(t * np.log(y+1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정답 클래스의 수가 2 이상일 때의 교차 엔트로피 오차를 구하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_batch(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y. reshape(1, y.size)\n",
    "        \n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t]+ 1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 왜 손실함수가 필요한가?\n",
    "\n",
    "- 왜 정확도를 놔두고 손실함수에 주목하는가?\n",
    "- 신경망 학습에서는 손실함수를 최소화 하는 값을 찾기 위해 매개변수의 미분을 계산하고, 미분을 단서로 매개변수 값을 갱신한다.   \n",
    "- 하지만, 정확도를 지표로 할 경우 매개변수 미분이 대부분의 장소에서 0이 되므로 개선점을 찾기 어렵다.    \n",
    "Q. 왜 정확도의 경우 매개변수 미분이 대부분의 장소에서 0이 되는걸까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수치 미분      \n",
    "\n",
    "- 미분 : '특정 순간'의 변화량   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
