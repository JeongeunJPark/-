{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 파이썬으로 말뭉치 전처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 파이썬의 대화 모드를 이용하여 매우 작은 텍스트 데이터(말뭉치)를 전처리해보자.        \n",
    "- 전처리 : 텍스트 데이터를 단어로 분할하고, 분할된 단어들을 단어 ID 목록으로 변환하는 일    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you said goodbye and i say hello .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"You said goodbye and I say hello.\"\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "# '.' 을 split을 위해 '공백.'으로 대체\n",
    "# 정규표현식을 사용하여 이 방법을 대체할 수 있음.\n",
    "# 정규표현식 모듈인 re를 import하고 re.split('(\\\\+)?'. text)로 단어단위로 분할 가능하다.\n",
    "text = text.replace('.', ' .')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'said', 'goodbye', 'and', 'i', 'say', 'hello', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 단어를 그대로 활용하기 어려우므로, 단어에 ID를 부여하고 이를 딕셔너리로 짝지어준다.\n",
    "\n",
    "words_to_id = {}\n",
    "id_to_words = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in words_to_id:\n",
    "        new_id = len(words_to_id)\n",
    "        \n",
    "        words_to_id[word] = new_id\n",
    "        id_to_words[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'you',\n",
       " 1: 'said',\n",
       " 2: 'goodbye',\n",
       " 3: 'and',\n",
       " 4: 'i',\n",
       " 5: 'say',\n",
       " 6: 'hello',\n",
       " 7: '.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0,\n",
       " 'said': 1,\n",
       " 'goodbye': 2,\n",
       " 'and': 3,\n",
       " 'i': 4,\n",
       " 'say': 5,\n",
       " 'hello': 6,\n",
       " '.': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#단어 ID 목록으로 변경\n",
    "import numpy as np\n",
    "\n",
    "corpus = [words_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이를 모아서 preprocessing이라는 함수로 구현\n",
    "def preprocessing(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' .')\n",
    "\n",
    "    words = text.split(' ')\n",
    "    \n",
    "    words_to_id = {}\n",
    "    id_to_words = {}\n",
    "\n",
    "    for word in words:\n",
    "        if word not in words_to_id:\n",
    "            new_id = len(words_to_id)\n",
    "        \n",
    "            words_to_id[word] = new_id\n",
    "            id_to_words[new_id] = word\n",
    "            \n",
    "    corpus = np.array([words_to_id[w] for w in words])\n",
    "    \n",
    "    return corpus, words_to_id, id_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You are my celebrity\"\n",
    "\n",
    "corpus, words_to_id, id_to_words = preprocessing(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you': 0, 'are': 1, 'my': 2, 'celebrity': 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus\n",
    "words_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 단어의 분산 표현             \n",
    "\n",
    "- 색에 이름을 붙일 때를 떠올려 보자. '번트 레나' 보다는 RGB(n, m, t)가 색상을 더 명확히 할 수 있다.   \n",
    "- 색끼리의 관련성도 RGB가 더 쉽게 판단 가능하고, 정량화하기 쉽다.     \n",
    "---\n",
    "- 그럼 단어도 이렇게 표현할 수 있을까?     \n",
    "- __분산 표현__ : '단어의 의미'를 정확하게 파악할 수 있는 벡터 표현   \n",
    "- __단어를 고정 길이의 밀집 벡터로 표현한다.(대부분의 원소가 0이 아닌 실수인 벡터)__   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 분포 가설        \n",
    "\n",
    "- __분포 가설__ : '단어의 의미는 주변 단어에 의해 형성된다'      \n",
    "- 단어 자체는 의미가 없고, 그 단어가 사용된 맥락이 의미를 형성한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![윈도우](./PostingPic/FBTT_2/윈도우.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맥락의 크기(주변 단어를 몇 개 표함할지) == 윈도우 크기      \n",
    "- 윈도우 크기가 2이면 좌, 우 각 2개씩이 포함된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 동시발생 행렬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분포 가설에 기초해 단어를 벡터로 만들어내려면 어떻게 해야 할까?         \n",
    "- 주변 단어를 '세어보는' 방법. => 통계기반 기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ca2e91a7638d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hi, Hello, greetings\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('')\n",
    "import numpy as np\n",
    "from common.util import preprocessing\n",
    "\n",
    "text = \"Hi, Hello, greetings\"\n",
    "corpus , word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![동시발생 행렬](./PostingPic/FBTT_2/동시발생행렬.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동시발생 행렬을 손으로 직접 입력해보자..!\n",
    "\n",
    "C = np.array([\n",
    "    [0,1,0,0,0,0,0],\n",
    "    [1,0,1,0,1,1,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,0,1,0,1,0,0],\n",
    "    [0,1,0,1,0,0,0],\n",
    "    [0,1,0,0,0,0,1],\n",
    "    [0,0,0,0,0,1,0]\n",
    "], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 아이디가 0인 단어의 벡터 표현\n",
    "print(C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#동시발생 행렬을 자동화\n",
    "def create_co_matrix(corpus, voca_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((voca_size, voca_size), dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size+1):\n",
    "            left_index = idx-i\n",
    "            right_index = idx+i\n",
    "            \n",
    "            if(left_index >=0):\n",
    "                left_word_id = corpus[left_index]\n",
    "                co_matrix[word_id, left_word_id] +=1\n",
    "                \n",
    "            if(right_index < corpus_size):\n",
    "                right_word_id = corpus[right_index]\n",
    "                co_matrix[word_id, right_word_id] +=1\n",
    "                \n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터 간 유사도       \n",
    "\n",
    "- 단어 벡터의 유사도를 나타낼 때는 코사인 유사도를 자주 이용한다.    \n",
    "- 코사인 유사도 ==> \"두 벡터가 가리키는 방향이 얼마나 비슷한가\" 두 단어가 완전히 같다면 1, 완전히 다르면 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy 배열인 x,y\n",
    "\n",
    "def cos_similarity(x, y):\n",
    "    nx = x/np.sqrt(np.sum(x**2))\n",
    "    ny = y/np.sqrt(np.sum(y**2))\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 경우 인수로 제로 벡터(원수가 모두 0인 벡터)가 들어오면 0으로 나누기 오류가 발생함       \n",
    "- 이를 해결하기 위해 분모에 아주아주아주 작은 벡터를 더한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy 배열인 x,y\n",
    "\n",
    "def cos_similarity(x, y, esp=1e-8):\n",
    "    nx = x/ (np.sqrt(np.sum(x**2)) +esp)\n",
    "    ny = y/ (np.sqrt(np.sum(y**2)) +esp)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 유사 단어의 랭킹 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'common'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c4b7bafa8bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproprocessing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_co_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"You say goodbye and I say hello.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'common'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from common.util import proprocessing, create_co_matrix, cos_similarity\n",
    "\n",
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocessing(text)\n",
    "\n",
    "voca_size = len(word_to_id)\n",
    "\n",
    "C = create_co_matrix(corpus, voca_size)\n",
    "\n",
    "c0 = C[word_to_id['you']]\n",
    "c1 = C[word_to_id['i']]\n",
    "\n",
    "print(cos_similarity(c0, c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 가장 유사도가 높은 순서대로 출력\n",
    "\n",
    "1. 검색어의 단어 벡터를 꺼낸다.\n",
    "2. 검색어의 단어 벡터와 다른 모든 단어 벡터와의 코사인 유사도를 각각 구한다.\n",
    "3. 계산한 코사인 유사도를 기준으로 값이 높은 순서대로 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x.argsort() 내용을 오름차순으로 정렬한 후, 해당하는 인덱스를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
