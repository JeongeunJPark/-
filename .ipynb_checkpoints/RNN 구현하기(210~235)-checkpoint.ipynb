{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 RNN 구현            \n",
    "\n",
    "### 1. 목표\n",
    "\n",
    "- 우리가 지금부터 구현할 것은 결국 '가로 방향으로 성장한 신경망'!       \n",
    "\n",
    "![rnn1](./PostingPic/rnn_1.png)       \n",
    "\n",
    "\n",
    "- 길이가 T인 시계열 데이터를 받아서, 각 시각의 은닉 상태를 T개 출력한다.\n",
    "- 상하 방향의 입력과, 출력을 하나로 묶으면 옆으로 늘어선 일련의 계층을 하나의 계층으로 간주할 수 있다.  \n",
    "- 즉, 묶어주는 $Xs$ 로 묶여진 $Hs$를 출력하는 하나의 계층"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 구현           \n",
    "\n",
    "- 한 단위의 RNN 클래스를 구현해보자.    \n",
    "- 여기서, 우리는 데이터를 미니배치로 처리한다.      \n",
    "\n",
    "![rnn2](./PostingPic/rnn_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ht(한 계층의 출력) = tahn(ht-1 * Wh + Xt * Wx + b)      \n",
    "\n",
    "> - h(t-1) * Wh : 이전 단계의 h값과, Wh(h 출력을 내기 위한 가중치 세트)      \n",
    "> - Xt * Wx : 현재 단계의 인풋 Xt와 Wx(h값을 내기 위한 가중치 세트)        \n",
    "> - b : 편향    \n",
    "\n",
    "##### 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    #파라미터로 Wx, Wh의 두 값이 필요\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params[Wx, Wh, b]\n",
    "        \n",
    "        #가중치는 요렇게\n",
    "        #동일 크기의 제로 행렬을 만들어준다.\n",
    "        self.grads[np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        \n",
    "        #다음 계층으로 넘어갈 때 기억해야 할 값인데\n",
    "        #현재는 아무것도 알 수 없으므로 None\n",
    "        self.cache = None\n",
    "        \n",
    "    #순방향 전파\n",
    "    #x(각 시각별 인풋)\n",
    "    #h_prev(이전 계층의 출력)\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        \n",
    "        # 이 부분이 바로 그 그림입니다\n",
    "        t = np.matmul(h_prev, Wh) + np.matmul(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        \n",
    "        #이번 층에서 나온 결과값인 h_next를 리턴\n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![순전파그래프](./PostingPic/순전파그래프.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 역전파   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순전파일때는 앞서있었던 그 값\n",
    "# 역전파일때는 거꾸로 온겁니다!\n",
    "def backward(self, dh_next):\n",
    "    Wx, Wh, b = self.params\n",
    "    x, h_prev, h_next = self.cache\n",
    "    \n",
    "    dt = dh_next * (1-h_next ** 2)\n",
    "    \n",
    "    #편향의 d\n",
    "    db = np.sum(dt, axis=0)\n",
    "    \n",
    "    #Wh의 d\n",
    "    dWh = np.matmul(h_prev.T, dt)\n",
    "    \n",
    "    #h_prev의 d\n",
    "    dh_prev = np.matmul(dt, Wh.T)\n",
    "    \n",
    "    #Wx의 d\n",
    "    dWx = np.matmul(x.T, dt)\n",
    "    \n",
    "    #x의 d\n",
    "    dx = np.matmul(dt, Wx.T)\n",
    "    \n",
    "    #전달해야 할 기울기 Wx, Wh, b\n",
    "    self.grad[0][] = dWx\n",
    "    self.grad[1][] = dWh\n",
    "    self.grad[2][] = db\n",
    "    \n",
    "    return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.2 TimeRnn계층의 구현                 \n",
    "\n",
    "- Time Rnn계층 == Rnn * T개         \n",
    "- Rnn 계층의 은닉 상태인 h를 인스턴스 변수로 사용    \n",
    "\n",
    "![TimeRnn](./PostingPic/TimeRnn.png)            \n",
    "\n",
    "\n",
    "- 이 책에서는 '은닉 상태를 인계받을지'를 stateful 이라는 인수로 조정한다.      \n",
    "- Stateful Stateless\n",
    "\n",
    "![예제](./PostingPic/State.png)             \n",
    "\n",
    "\n",
    "##### 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "        \n",
    "        #이번 계층의 결과물, h의 기울기\n",
    "        self.h, self.dh = None, None\n",
    "        \n",
    "        #연결 상태는 어떻게 할거니?\n",
    "        self.stateful = stateful\n",
    "        \n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 계층은 여러 개의 x, h를 묶은 것이라는 것을 기억하자!\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        #N = 미니배치, T=rnn개수, D=입력 벡터의 차원 수 \n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        #레이어를 리스트로 담아둘거야.\n",
    "        self.layers = []\n",
    "        \n",
    "        #결과물을 담아둘 수 있는 행렬을 만듦\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        #스테이트가 true이고 h가 설정되어 있지 않으면\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            # *는 리스트의 원소를 추출하여 메서드의 인수로 전달한다.\n",
    "            # self.params = [Wx, Wh, b]\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:,t,:], self.h)\n",
    "            \n",
    "            #  N, T, H\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이렇게](./PostingPic/rnn_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 역전파           \n",
    "\n",
    "- 우리는 Truncated BPTT(잘린 BPTT)를 수행하므로, 이전 시각 역전파는 기억하지 않는다. (한 단위에서 끝)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이렇게](./PostingPic/TimeRnn_bptt.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(self, dhs):\n",
    "    Wx, Wh, b = self.params\n",
    "    \n",
    "    # 기울기 셰입\n",
    "    N, T, H = dhs.shpae\n",
    "    D, H = Wx.shape\n",
    "    \n",
    "    #모든 기울기는 0으로 초기화 세팅\n",
    "    dxs = np.empty((N,T,D), dtype='f')\n",
    "    dh = 0\n",
    "    grads =[0,0,0]\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        layer = self.layers[t]\n",
    "        \n",
    "        dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "        \n",
    "        #전체 인풋 xs에 대한 기울기\n",
    "        dxs = dx\n",
    "        \n",
    "        for i, grad in enumerate(layer.grads):\n",
    "            grads[i] +=  grad\n",
    "            \n",
    "    for i, grad in enumerate(layer.grads):\n",
    "        self.grads[i][] = grad\n",
    "    \n",
    "    self.dh = dh\n",
    "    \n",
    "    return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단, 순전파 시에 갈라졌던 부분들을 합산해서 적용해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 시계열 데이터 처리 계층 구현          \n",
    "\n",
    "- RNNLM (RNN + LM)       \n",
    "\n",
    "![모델](./PostingPic/언어모델.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 층 : 단어 ID를 단어 벡터(분산표현)으로 변환         \n",
    "- RNN : 우리가 지금까지 한 것      \n",
    "- Affine, Softmax : 마지막 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![여기](./PostingPic/5-26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Say를 보자구   \n",
    "- hello, goodbye 둘 다 확률이 높게 나옴(둘 다 정답이 될 만한 단어들)        \n",
    "- You say goodbye and I say hello  의 순서를 __기억하고__ 있다는 것         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Time embedding 구현, Time Affine 구현              \n",
    "\n",
    "[전체 코드는 여기에](https://github.com/WegraLee/deep-learning-from-scratch-2/blob/master/common/time_layers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. simple RnnLm                \n",
    "\n",
    "- 4개의 계층을 쌓은 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.time_layers import *\n",
    "\n",
    "#F: 포트란형식 Index 순서, 앞 차원부터 변경하고 뒷 쪽 차원을 변경\n",
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 가중치 초기화\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN과 Affine 계층에서 Xavier 초깃값 (6.2 가중치의 초깃값)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 언어모델의 평가                 \n",
    "\n",
    "##### 퍼플렉시티(Perplexity, 혼란도)           \n",
    "- 확률의 역수         \n",
    "- 각 언어 벡터에 대한 정답 확률을, 거꾸로 생각해보는 것       \n",
    "\n",
    "![퍼플렉시티](./PostingPic/퍼플렉시티.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 직관적으로 생각할 때, 분기 수로 해석할 수 있다.           \n",
    "- 내가 고를 수 있는 선택지가 몇 개인가?             \n",
    "- 정답 확률이 낮은 경우(0.1퍼일 때) = > 퍼플렉시티는 10. 이는 곧 선택가능한 선택지 10개를 주며 이중에 정답이 있어~ 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 질문 모음\n",
    "\n",
    "(진호) \n",
    "Q1. Truncate BPTT 적당한 길이란 어떻게 되는지? BPTT 어떤 기준으로 잡을 수 있을까요?\n",
    "A1. https://machinelearningmastery.com/truncated-backpropagation-through-time-in-keras/ 의 2번을 보시면 어떨까요?\n",
    "\n",
    "(병학)\n",
    "Q1. RNN 계층 모두가 실제로는 '같은 계층'이라고 하는것이 X0 ~Xt 가 RNN층을 만나 h0~ht로 변하는기 때문에 같은 계층이라고 하는 것인가요? \n",
    "A1. (스레드로 알려주세요!)\n",
    "\n",
    "(창원)\n",
    "Q1.(p202) RNN 계층 모두가 실제로는 '같은 계층': 그림 5-8에 있는 것은 하나의 계층인가?\n",
    "A1. (스레드로 알려주세요!)\n",
    "\n",
    "Q2.(p202) 식 5.9에서 tanh를 사용하는 이유?\n",
    "A2. The vanishing gradient problem is the main problem in RNN. Also, to keep the gradient in the linear region of the activation function, we need a function whose second derivative can sustain for a long range before going to zero. Tanh is pretty good with these properties. 라고 합니다.\n",
    "\n",
    "Q3. (p204) 시간 크기가 커지면 역전파 시의 기울기가 불안정해지는 이유?\n",
    "A3. 미분이기 때문에 점점 줄어들고, 멀리까지 값이 퍼지기 어렵기 때문인 것 같습니다.\n",
    "\n",
    "Q4.(p208~209) Truncated BPTT의 미니배치 학습에 대해 설명해주세요\n",
    "A4. (스레드로 알려주세요!)\n",
    "\n",
    "(상미)\n",
    "Q1.(p205) 신경망 하나씩 통과할 때마다, 기울기 값이 작아져 불안정해진다고 이해했습니다. 이때 '역전파 시 기울기가 불안정해진다', '역전파 일때만 끊어준다'와 같이 역전파에만 해당시키는 이유가 있을까요?\n",
    "A1. (스레드로 알려주세요!)\n",
    "\n",
    "(창호)\n",
    "Q1. hidden state에는 가중치가 왜 2개 필요한가? 그것이 의미하는 게 무엇인가?\n",
    "A1. (스레드로 알려주세요!)\n",
    "\n",
    "Q2. 자연어를 반드시 시계열 데이터라고만 봐야할까?\n",
    "A2. 문맥의 순서가 있으므로 시계열로 볼 수 있을 거라 생각한다.\n",
    "    현재에 가치를 더 둘건지 과거의 데이터에 가치를 더 둘 건지?\n",
    "\n",
    "(준형)\n",
    "Q1. (p201) RNN 순환구조에 입력되는 데이터(x0, x1, x2....xt) 각각은 말뭉치 전체 데이터인가요 아니면 맥락인가요? \n",
    "A1.193쪽 그림 5-2가 도움이 될 것 같습니다.\n",
    "   Exploration 작사가 노드를 보시면 좋을 것 같습니다!\n",
    "\n",
    "(정은) \n",
    "Q1. 왜 Xavier 초깃값을 썼을까?     \n",
    "A1. −m과 +m 사이의 균등 분포를 의미합니다. 세이비어 초기화는 여러 층의 기울기 분산 사이에 균형을 맞춰서 특정 층이 너무 주목을 받거나 다른 층이 뒤쳐지는 것을 막습니다. Xavier 초기화는 시그모이드 함수나 하이퍼볼릭 탄젠트 함수와 같은 S자 형태인 활성화 함수와 함께 사용할 경우에는 좋은 성능을 보입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(상효) \n",
    "Q1. 사람의 기억상실,운동의 소실과 같이 시간이 흐름에 따라 발생하는 퇴행성 질환의 경우 \n",
    "시계열 데이터를 다루는 RNN모델의 구조의처리방식과 상관관계가 있을까요 ? \n",
    "상관관계가 있다면 반대로 RNN모델을 이용하여 퇴행성 뇌질환의 조기 진단이 가능할까요 ?\n",
    "\n",
    "A1. https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002468561\n",
    "2. 노년기 3대 질환 중 하나인 파킨슨병은 환자의 70 % 이상이 음성 장애를 앓고 있으며 최근 음성 신호를 통한파킨슨병의 진단 방법들이 고안되고 있다. 본 논문에서는 음성 특징을 이용한 심층 잔류 순환 신경망 기반의 파킨슨병진단 방식을 제안한다. 제안하는 방식에서는 파킨슨병 진단을 위한 음성 특징을 선택하고 이를 심층 잔류 순환 신경망에 적용하여 파킨슨병 환자를 식별한다. 제안하는 심층 잔류 순환 신경망은 심층 순환 신경망에 잔류 학습 방식을 결합한 알고리즘으로 파킨슨병 진단에서 기존의 식별 알고리즘보다 더 높은 인식률을 보인다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
